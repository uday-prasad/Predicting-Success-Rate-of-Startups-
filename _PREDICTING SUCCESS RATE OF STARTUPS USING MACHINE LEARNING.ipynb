pip install pandas

pip install seaborn

pip install matplotlib

pip install scikit-learn

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
# Load your startup dataset (team 6)
data = pd.read_csv('startup_data.csv')

data
data.isnull().sum()
data = data.drop(columns=['age_first_milestone_year', 'age_last_milestone_year','Unnamed: 6','closed_at'])

data.isnull().sum()
# Generate synthetic data for demonstration
np.random.seed(0)
X = np.random.rand(100, 2)  # Two features for simplicity
y = np.random.choice([0, 1], 100)  # Binary target variable
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
# Assuming your DataFrame is named 'data'
categorical_columns = data.select_dtypes(include=['object', 'category']).columns

# Print the list of categorical columns
print("Categorical Columns:")
print(categorical_columns)
column_to_keep = 'status'
# Select columns with data type 'object' (strings)
string_columns = data.select_dtypes(include=['object'])

# List of columns to drop (excluding the one to keep)
columns_to_drop = [col for col in string_columns.columns if col != column_to_keep]

# Drop the selected string columns (excluding the one to keep)
data = data.drop(columns_to_drop, axis=1)

data
# Encode categorical variables
label_encoders = {}
categorical_columns = ['status']
for col in categorical_columns:
    le = LabelEncoder()
    data[col] = le.fit_transform(data[col])
    label_encoders[col] = le
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Standardize features (not necessary for this synthetic data)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
# Define the classifiers
classifiers = {
    'Logistic Regression': LogisticRegression(),
    'Gradient Boosting': GradientBoostingClassifier(),
    'Random Forest': RandomForestClassifier(),
    'Decision Tree': DecisionTreeClassifier(),
    'MLP Neural Network': MLPClassifier(max_iter=1000)
}
# Train and test each classifier, and print its accuracy for both the training and testing data
for clf_name, clf in classifiers.items():
    clf.fit(X_train, y_train)
    y_train_pred = clf.predict(X_train)
    y_test_pred = clf.predict(X_test)

    train_accuracy = accuracy_score(y_train, y_train_pred)
    test_accuracy = accuracy_score(y_test, y_test_pred)

    print(f'{clf_name}:')
    print(f'Training Accuracy: {train_accuracy:.2f}')
    print(f'Testing Accuracy: {test_accuracy:.2f}')
    print()
from sklearn.metrics import roc_auc_score
# Initialize and train each model
models = {
    'Logistic Regression': LogisticRegression(),
    'Gradient Boosting': GradientBoostingClassifier(),
    'Random Forest': RandomForestClassifier(),
    'Decision Tree': DecisionTreeClassifier(),
    'MLP Neural Network': MLPClassifier(max_iter=1000)
}
# Assuming 'X_train' and 'y_train' contain your training data
for model_name, model in models.items():
    model.fit(X_train, y_train)

# Assuming 'X_test' is your test data
y_true = y_test  # Replace 'y_true' with your actual true labels

# Calculate AUC for each model
auc_values = {model_name: roc_auc_score(y_true, model.predict_proba(X_test)[:, 1]) for model_name, model in models.items()}

# Print AUC values for each model
for model_name, auc in auc_values.items():
    print(f'{model_name} - AUC: {auc:.2f}')
models = {
    'Logistic Regression': LogisticRegression(),
    'Gradient Boosting': GradientBoostingClassifier(),
    'MLP Neural Network': MLPClassifier(max_iter=1000),
    'Random Forest': RandomForestClassifier(),
    'Decision Tree': DecisionTreeClassifier()
}

trained_models = {}
for model_name, model in models.items():
    model.fit(X_train, y_train)
    trained_models[model_name] = model

# 4. Evaluate Models
model_results = {}
for model_name, model in trained_models.items():
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(y_test, y_pred)
    model_results[model_name] = {
        'Accuracy': accuracy,
        'Classification Report': report
    }
# 5. Compare Model Performance
for model_name, results in model_results.items():
    print(f"Model: {model_name}")
    print(f"Accuracy: {results['Accuracy']}")
    print(f"Classification Report:\n{results['Classification Report']}")
    print("=" * 50)
import matplotlib.pyplot as plt
model_names = ['Logistic Regression', 'Gradient Boosting', 'MLP Neural Network', 'Random Forest', 'Decision Tree']
model_accuracies = [0.5, 0.45, 0.3, 0.45, 0.3]  # Accuracy values corresponding to each model

plt.figure(figsize=(8, 4))
plt.barh(model_names, model_accuracies, color='lightpink')
plt.xlabel('Accuracy')
plt.title('Model Comparison')
plt.gca().invert_yaxis()
plt.show()
import seaborn as sns
# Define a custom color palette
custom_palette = {'Closed': 'red', 'Acquired': 'green'}

# Assuming 'df' is your DataFrame and 'status' is the target variable
plt.figure(figsize=(8, 6))
sns.countplot(data=data, x='status', hue=data['status'].map({0: 'Closed', 1: 'Acquired'}), dodge=False, 
              palette=custom_palette)

plt.xlabel('Status')
plt.ylabel('Count')
plt.title('Distribution of Status')
plt.legend(title='Status', labels=['Closed', 'Acquired'])
plt.show()
